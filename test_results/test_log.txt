[12:53:10.498] Starting testing with args: {'checkpoint': 'checkpoints_enhanced\\best_model_Parse_R50-ViT-B_16.pth', 'vit_name': 'R50-ViT-B_16', 'img_size': 224, 'n_skip': 3, 'vit_patches_size': 16, 'dataset': 'Parse', 'list_dir': './lists/lists_Parse', 'root_path': './DATA', 'output_dir': './test_results', 'seed': 1234, 'skip_empty_slices': False, 'use_tta': True, 'tta_num': 4}
[12:53:10.514] Using device: cuda
[12:53:10.516] Initializing model: R50-ViT-B_16
[12:53:11.248] Loading model from checkpoint: checkpoints_enhanced\best_model_Parse_R50-ViT-B_16.pth
[12:53:11.464] Failed to load directly into wrapped model: Error(s) in loading state_dict for VisionTransformer:
	Missing key(s) in state_dict: "decoder.blocks.0.attention_gate.W_g.0.weight", "decoder.blocks.0.attention_gate.W_g.0.bias", "decoder.blocks.0.attention_gate.W_g.1.weight", "decoder.blocks.0.attention_gate.W_g.1.bias", "decoder.blocks.0.attention_gate.W_g.1.running_mean", "decoder.blocks.0.attention_gate.W_g.1.running_var", "decoder.blocks.0.attention_gate.W_x.0.weight", "decoder.blocks.0.attention_gate.W_x.0.bias", "decoder.blocks.0.attention_gate.W_x.1.weight", "decoder.blocks.0.attention_gate.W_x.1.bias", "decoder.blocks.0.attention_gate.W_x.1.running_mean", "decoder.blocks.0.attention_gate.W_x.1.running_var", "decoder.blocks.0.attention_gate.psi.0.weight", "decoder.blocks.0.attention_gate.psi.0.bias", "decoder.blocks.0.attention_gate.psi.1.weight", "decoder.blocks.0.attention_gate.psi.1.bias", "decoder.blocks.0.attention_gate.psi.1.running_mean", "decoder.blocks.0.attention_gate.psi.1.running_var", "decoder.blocks.1.attention_gate.W_g.0.weight", "decoder.blocks.1.attention_gate.W_g.0.bias", "decoder.blocks.1.attention_gate.W_g.1.weight", "decoder.blocks.1.attention_gate.W_g.1.bias", "decoder.blocks.1.attention_gate.W_g.1.running_mean", "decoder.blocks.1.attention_gate.W_g.1.running_var", "decoder.blocks.1.attention_gate.W_x.0.weight", "decoder.blocks.1.attention_gate.W_x.0.bias", "decoder.blocks.1.attention_gate.W_x.1.weight", "decoder.blocks.1.attention_gate.W_x.1.bias", "decoder.blocks.1.attention_gate.W_x.1.running_mean", "decoder.blocks.1.attention_gate.W_x.1.running_var", "decoder.blocks.1.attention_gate.psi.0.weight", "decoder.blocks.1.attention_gate.psi.0.bias", "decoder.blocks.1.attention_gate.psi.1.weight", "decoder.blocks.1.attention_gate.psi.1.bias", "decoder.blocks.1.attention_gate.psi.1.running_mean", "decoder.blocks.1.attention_gate.psi.1.running_var", "decoder.blocks.2.attention_gate.W_g.0.weight", "decoder.blocks.2.attention_gate.W_g.0.bias", "decoder.blocks.2.attention_gate.W_g.1.weight", "decoder.blocks.2.attention_gate.W_g.1.bias", "decoder.blocks.2.attention_gate.W_g.1.running_mean", "decoder.blocks.2.attention_gate.W_g.1.running_var", "decoder.blocks.2.attention_gate.W_x.0.weight", "decoder.blocks.2.attention_gate.W_x.0.bias", "decoder.blocks.2.attention_gate.W_x.1.weight", "decoder.blocks.2.attention_gate.W_x.1.bias", "decoder.blocks.2.attention_gate.W_x.1.running_mean", "decoder.blocks.2.attention_gate.W_x.1.running_var", "decoder.blocks.2.attention_gate.psi.0.weight", "decoder.blocks.2.attention_gate.psi.0.bias", "decoder.blocks.2.attention_gate.psi.1.weight", "decoder.blocks.2.attention_gate.psi.1.bias", "decoder.blocks.2.attention_gate.psi.1.running_mean", "decoder.blocks.2.attention_gate.psi.1.running_var". 
	size mismatch for decoder.blocks.0.conv1.0.weight: copying a param with shape torch.Size([256, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 576, 3, 3]).
	size mismatch for decoder.blocks.2.conv1.0.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 640, 3, 3]).. Trying alternate approach.
[12:53:11.470] Failed to load checkpoint: Error(s) in loading state_dict for VisionTransformer:
	Missing key(s) in state_dict: "decoder.blocks.0.attention_gate.W_g.0.weight", "decoder.blocks.0.attention_gate.W_g.0.bias", "decoder.blocks.0.attention_gate.W_g.1.weight", "decoder.blocks.0.attention_gate.W_g.1.bias", "decoder.blocks.0.attention_gate.W_g.1.running_mean", "decoder.blocks.0.attention_gate.W_g.1.running_var", "decoder.blocks.0.attention_gate.W_x.0.weight", "decoder.blocks.0.attention_gate.W_x.0.bias", "decoder.blocks.0.attention_gate.W_x.1.weight", "decoder.blocks.0.attention_gate.W_x.1.bias", "decoder.blocks.0.attention_gate.W_x.1.running_mean", "decoder.blocks.0.attention_gate.W_x.1.running_var", "decoder.blocks.0.attention_gate.psi.0.weight", "decoder.blocks.0.attention_gate.psi.0.bias", "decoder.blocks.0.attention_gate.psi.1.weight", "decoder.blocks.0.attention_gate.psi.1.bias", "decoder.blocks.0.attention_gate.psi.1.running_mean", "decoder.blocks.0.attention_gate.psi.1.running_var", "decoder.blocks.1.attention_gate.W_g.0.weight", "decoder.blocks.1.attention_gate.W_g.0.bias", "decoder.blocks.1.attention_gate.W_g.1.weight", "decoder.blocks.1.attention_gate.W_g.1.bias", "decoder.blocks.1.attention_gate.W_g.1.running_mean", "decoder.blocks.1.attention_gate.W_g.1.running_var", "decoder.blocks.1.attention_gate.W_x.0.weight", "decoder.blocks.1.attention_gate.W_x.0.bias", "decoder.blocks.1.attention_gate.W_x.1.weight", "decoder.blocks.1.attention_gate.W_x.1.bias", "decoder.blocks.1.attention_gate.W_x.1.running_mean", "decoder.blocks.1.attention_gate.W_x.1.running_var", "decoder.blocks.1.attention_gate.psi.0.weight", "decoder.blocks.1.attention_gate.psi.0.bias", "decoder.blocks.1.attention_gate.psi.1.weight", "decoder.blocks.1.attention_gate.psi.1.bias", "decoder.blocks.1.attention_gate.psi.1.running_mean", "decoder.blocks.1.attention_gate.psi.1.running_var", "decoder.blocks.2.attention_gate.W_g.0.weight", "decoder.blocks.2.attention_gate.W_g.0.bias", "decoder.blocks.2.attention_gate.W_g.1.weight", "decoder.blocks.2.attention_gate.W_g.1.bias", "decoder.blocks.2.attention_gate.W_g.1.running_mean", "decoder.blocks.2.attention_gate.W_g.1.running_var", "decoder.blocks.2.attention_gate.W_x.0.weight", "decoder.blocks.2.attention_gate.W_x.0.bias", "decoder.blocks.2.attention_gate.W_x.1.weight", "decoder.blocks.2.attention_gate.W_x.1.bias", "decoder.blocks.2.attention_gate.W_x.1.running_mean", "decoder.blocks.2.attention_gate.W_x.1.running_var", "decoder.blocks.2.attention_gate.psi.0.weight", "decoder.blocks.2.attention_gate.psi.0.bias", "decoder.blocks.2.attention_gate.psi.1.weight", "decoder.blocks.2.attention_gate.psi.1.bias", "decoder.blocks.2.attention_gate.psi.1.running_mean", "decoder.blocks.2.attention_gate.psi.1.running_var". 
	size mismatch for decoder.blocks.0.conv1.0.weight: copying a param with shape torch.Size([256, 1024, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 576, 3, 3]).
	size mismatch for decoder.blocks.2.conv1.0.weight: copying a param with shape torch.Size([64, 192, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 640, 3, 3]).
